# final_long_running_benchmark.py
import asyncio
import time
import aiohttp
import json
import csv
import psutil
import subprocess
import pandas as pd
from datetime import datetime, timedelta
import argparse
import os
import numpy as np
from collections import deque

class FinalLongRunningBenchmark:
    def __init__(self, args):
        self.args = args
        self.performance_data = []
        self.batch_performance_data = []  # æ¯æ‰¹è¯·æ±‚çš„æ€§èƒ½æ•°æ®
        self.start_time = None
        self.end_time = None
        self.request_counter = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.total_tokens = 0
        
        # æ‰¹å¤„ç†ç»Ÿè®¡
        self.batch_size = args.batch_size
        self.current_batch_requests = 0
        self.current_batch_tokens = 0
        self.current_batch_start_time = None
        self.last_batch_time = None
        
        # æ»‘åŠ¨çª—å£ç»Ÿè®¡
        self.recent_rps = deque(maxlen=10)  # æœ€è¿‘10æ‰¹çš„RPS
        self.recent_tps = deque(maxlen=10)  # æœ€è¿‘10æ‰¹çš„TPS
        
        # å†å²æ•°æ®å­˜å‚¨ï¼ˆç”¨äºç”ŸæˆæŠ¥å‘Šï¼‰
        self.hourly_performance = []
        
    async def get_gpu_metrics(self):
        """è·å–GPUæŒ‡æ ‡"""
        try:
            result = subprocess.check_output([
                'nvidia-smi', 
                '--query-gpu=timestamp,temperature.gpu,power.draw,utilization.gpu,utilization.memory',
                '--format=csv,noheader,nounits'
            ], encoding='utf-8', timeout=10)
            
            lines = result.strip().split('\n')
            gpu_data = []
            
            for line in lines:
                parts = [part.strip() for part in line.split(',')]
                if len(parts) >= 5:
                    def safe_float(value):
                        try:
                            return float(value) if value and value != '[N/A]' else 0.0
                        except:
                            return 0.0
                    
                    gpu_data.append({
                        'timestamp': parts[0],
                        'gpu_temp': safe_float(parts[1]),
                        'power_draw': safe_float(parts[2]),
                        'gpu_util': safe_float(parts[3]),
                        'memory_util': safe_float(parts[4])
                    })
            
            return gpu_data
        except Exception as e:
            print(f"è·å–GPUæŒ‡æ ‡å¤±è´¥: {e}")
            return [{
                'timestamp': datetime.now().strftime("%Y/%m/%d %H:%M:%S"),
                'gpu_temp': 0,
                'power_draw': 0,
                'gpu_util': 0,
                'memory_util': 0
            }]
    
    async def get_system_metrics(self):
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk_io = psutil.disk_io_counters()
        
        return {
            'timestamp': datetime.now().strftime("%Y/%m/%d %H:%M:%S"),
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used_gb': memory.used / (1024**3),
            'memory_total_gb': memory.total / (1024**3),
            'disk_read_mb': disk_io.read_bytes / (1024**2) if disk_io else 0,
            'disk_write_mb': disk_io.write_bytes / (1024**2) if disk_io else 0
        }
    
    async def record_batch_performance(self):
        """è®°å½•ä¸€æ‰¹è¯·æ±‚çš„æ€§èƒ½æŒ‡æ ‡"""
        if self.current_batch_requests == 0:
            return
            
        current_time = time.time()
        batch_duration = current_time - self.current_batch_start_time
        
        # è®¡ç®—è¿™æ‰¹è¯·æ±‚çš„æ€§èƒ½
        batch_rps = self.current_batch_requests / batch_duration if batch_duration > 0 else 0
        batch_tps = self.current_batch_tokens / batch_duration if batch_duration > 0 else 0
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        gpu_metrics = await self.get_gpu_metrics()
        system_metrics = await self.get_system_metrics()
        
        batch_data = {
            'batch_timestamp': datetime.now().isoformat(),
            'elapsed_minutes': (current_time - self.start_time) / 60,
            'elapsed_hours': (current_time - self.start_time) / 3600,
            'batch_duration_seconds': batch_duration,
            'batch_requests': self.current_batch_requests,
            'batch_tokens': self.current_batch_tokens,
            'batch_rps': batch_rps,
            'batch_tps': batch_tps,
            'cumulative_requests': self.request_counter,
            'cumulative_tokens': self.total_tokens,
            'success_rate': self.successful_requests / max(self.request_counter, 1),
            **system_metrics
        }
        
        # æ·»åŠ GPUæŒ‡æ ‡
        if gpu_metrics:
            batch_data.update(gpu_metrics[0])
        
        self.batch_performance_data.append(batch_data)
        self.recent_rps.append(batch_rps)
        self.recent_tps.append(batch_tps)
        
        # è¾“å‡ºå®æ—¶æ€§èƒ½
        avg_rps = np.mean(self.recent_rps) if self.recent_rps else 0
        avg_tps = np.mean(self.recent_tps) if self.recent_tps else 0
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] "
              f"æ‰¹æ¬¡: {len(self.batch_performance_data):4d} | "
              f"ç¬æ—¶RPS: {batch_rps:6.2f} | "
              f"ç¬æ—¶TPS: {batch_tps:6.2f} | "
              f"å¹³å‡RPS: {avg_rps:6.2f} | "
              f"å¹³å‡TPS: {avg_tps:6.2f} | "
              f"GPU: {batch_data.get('gpu_temp', 0):3.0f}Â°C/{batch_data.get('gpu_util', 0):3.0f}% | "
              f"åŠŸè€—: {batch_data.get('power_draw', 0):5.1f}W | "
              f"æˆåŠŸç‡: {batch_data['success_rate']:5.1%}")
        
        # ä¿å­˜åˆ°CSV
        self.save_batch_metrics_to_csv()
        
        # é‡ç½®å½“å‰æ‰¹æ¬¡ç»Ÿè®¡
        self.current_batch_requests = 0
        self.current_batch_tokens = 0
        self.current_batch_start_time = current_time
        self.last_batch_time = current_time
    
    async def send_request(self, session, prompt, request_id):
        """å‘é€å•ä¸ªè¯·æ±‚"""
        payload = {
            "prompt": prompt,
            "max_tokens": self.args.max_tokens,
            "temperature": self.args.temperature,
            "stream": False
        }
        
        # å¦‚æœæ˜¯æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œè®°å½•å¼€å§‹æ—¶é—´
        if self.current_batch_requests == 0:
            self.current_batch_start_time = time.time()
        
        start_time = time.time()
        try:
            async with session.post(
                self.args.url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                end_time = time.time()
                latency = (end_time - start_time) * 1000
                
                if response.status == 200:
                    result = await response.json()
                    generated_text = result.get("text", [""])[0]
                    
                    # ä½¿ç”¨ç®€å•çš„åˆ†è¯æ–¹æ³•ä¼°ç®—tokenæ•°é‡
                    tokens_generated = len(generated_text.split())
                    
                    # æ›´æ–°å…¨å±€è®¡æ•°å™¨
                    self.request_counter += 1
                    self.successful_requests += 1
                    self.total_tokens += tokens_generated
                    
                    # æ›´æ–°æ‰¹æ¬¡è®¡æ•°å™¨
                    self.current_batch_requests += 1
                    self.current_batch_tokens += tokens_generated
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    if self.current_batch_requests >= self.batch_size:
                        await self.record_batch_performance()
                    
                    return {
                        "request_id": request_id,
                        "latency": latency,
                        "success": True,
                        "tokens_generated": tokens_generated
                    }
                else:
                    self.request_counter += 1
                    self.failed_requests += 1
                    self.current_batch_requests += 1
                    
                    if self.current_batch_requests >= self.batch_size:
                        await self.record_batch_performance()
                    
                    error_text = await response.text()
                    return {
                        "request_id": request_id,
                        "latency": latency,
                        "success": False,
                        "error": f"HTTP {response.status}: {error_text[:100]}"
                    }
        except asyncio.TimeoutError:
            end_time = time.time()
            self.request_counter += 1
            self.failed_requests += 1
            self.current_batch_requests += 1
            
            if self.current_batch_requests >= self.batch_size:
                await self.record_batch_performance()
            
            return {
                "request_id": request_id,
                "latency": (end_time - start_time) * 1000,
                "success": False,
                "error": "Timeout"
            }
        except Exception as e:
            end_time = time.time()
            self.request_counter += 1
            self.failed_requests += 1
            self.current_batch_requests += 1
            
            if self.current_batch_requests >= self.batch_size:
                await self.record_batch_performance()
            
            return {
                "request_id": request_id,
                "latency": (end_time - start_time) * 1000,
                "success": False,
                "error": str(e)
            }
    
    async def request_worker(self, session, prompt, worker_id, stop_event):
        """è¯·æ±‚å·¥ä½œçº¿ç¨‹"""
        request_id_base = worker_id * 1000000
        local_counter = 0
        
        while not stop_event.is_set():
            request_id = request_id_base + local_counter
            await self.send_request(session, prompt, request_id)
            local_counter += 1
            await asyncio.sleep(0.01)
    
    async def hourly_summary_collector(self, stop_event):
        """æ¯å°æ—¶æ±‡æ€»æ”¶é›†å™¨"""
        collect_interval = 3600  # 1å°æ—¶
        
        while not stop_event.is_set():
            await asyncio.sleep(collect_interval)
            
            current_time = time.time()
            elapsed_hours = (current_time - self.start_time) / 3600
            
            if self.batch_performance_data:
                # è·å–æœ€è¿‘ä¸€å°æ—¶çš„æ•°æ®
                recent_data = [d for d in self.batch_performance_data 
                             if d['elapsed_hours'] >= (elapsed_hours - 1) and d['elapsed_hours'] <= elapsed_hours]
                
                if recent_data:
                    df_hour = pd.DataFrame(recent_data)
                    
                    hourly_summary = {
                        'hour': int(elapsed_hours),
                        'timestamp': datetime.now().isoformat(),
                        'avg_rps': df_hour['batch_rps'].mean(),
                        'avg_tps': df_hour['batch_tps'].mean(),
                        'max_rps': df_hour['batch_rps'].max(),
                        'max_tps': df_hour['batch_tps'].max(),
                        'avg_gpu_temp': df_hour['gpu_temp'].mean(),
                        'max_gpu_temp': df_hour['gpu_temp'].max(),
                        'avg_power_draw': df_hour['power_draw'].mean(),
                        'max_power_draw': df_hour['power_draw'].max(),
                        'avg_gpu_util': df_hour['gpu_util'].mean(),
                        'total_requests': self.request_counter,
                        'total_tokens': self.total_tokens,
                        'success_rate': self.successful_requests / max(self.request_counter, 1)
                    }
                    
                    self.hourly_performance.append(hourly_summary)
                    
                    print(f"\nğŸ“Š ç¬¬{int(elapsed_hours)}å°æ—¶æ€§èƒ½æ±‡æ€»:")
                    print(f"   å¹³å‡RPS: {hourly_summary['avg_rps']:.2f}, å¹³å‡TPS: {hourly_summary['avg_tps']:.2f}")
                    print(f"   GPUæ¸©åº¦: {hourly_summary['avg_gpu_temp']:.1f}Â°C, åŠŸè€—: {hourly_summary['avg_power_draw']:.1f}W")
                    print(f"   ç´¯è®¡è¯·æ±‚: {hourly_summary['total_requests']:,}, æˆåŠŸç‡: {hourly_summary['success_rate']:.1%}")
    
    async def backup_metrics_collector(self, stop_event):
        """å¤‡ç”¨æŒ‡æ ‡æ”¶é›†å™¨ï¼ˆé˜²æ­¢é•¿æ—¶é—´æ²¡æœ‰æ‰¹æ¬¡å®Œæˆï¼‰"""
        collect_interval = 30  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
        
        while not stop_event.is_set():
            current_time = time.time()
            
            # å¦‚æœæœ‰æœªå®Œæˆçš„æ‰¹æ¬¡ä¸”è¶…è¿‡ä¸€å®šæ—¶é—´æ²¡æœ‰æ›´æ–°ï¼Œå¼ºåˆ¶è®°å½•
            if (self.current_batch_requests > 0 and 
                self.last_batch_time and 
                current_time - self.last_batch_time > 10):  # è¶…è¿‡10ç§’æ²¡æœ‰å®Œæˆæ‰¹æ¬¡
                await self.record_batch_performance()
            
            await asyncio.sleep(collect_interval)
    
    def save_batch_metrics_to_csv(self):
        """ä¿å­˜æ‰¹æ¬¡æ€§èƒ½æŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
        if not self.batch_performance_data:
            return
        
        filename = f"long_running_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            if self.batch_performance_data:
                fieldnames = self.batch_performance_data[0].keys()
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.batch_performance_data)
        
        return filename
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.batch_performance_data:
            print("æ²¡æœ‰æ”¶é›†åˆ°æ€§èƒ½æ•°æ®")
            return
        
        df = pd.DataFrame(self.batch_performance_data)
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        total_duration_hours = (self.end_time - self.start_time) / 3600
        
        report = {
            "test_configuration": {
                "duration_hours": self.args.duration_hours,
                "concurrent_workers": self.args.concurrent_workers,
                "batch_size": self.batch_size,
                "max_tokens": self.args.max_tokens,
                "temperature": self.args.temperature,
                "url": self.args.url
            },
            "test_timing": {
                "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
                "end_time": datetime.fromtimestamp(self.end_time).isoformat(),
                "actual_duration_hours": total_duration_hours
            },
            "request_statistics": {
                "total_requests": self.request_counter,
                "successful_requests": self.successful_requests,
                "failed_requests": self.failed_requests,
                "success_rate": self.successful_requests / max(self.request_counter, 1),
                "total_tokens": self.total_tokens
            },
            "performance_metrics": {
                "average_batch_rps": df['batch_rps'].mean(),
                "max_batch_rps": df['batch_rps'].max(),
                "average_batch_tps": df['batch_tps'].mean(),
                "max_batch_tps": df['batch_tps'].max(),
                "std_batch_rps": df['batch_rps'].std(),
                "std_batch_tps": df['batch_tps'].std(),
                "p95_batch_rps": df['batch_rps'].quantile(0.95),
                "p95_batch_tps": df['batch_tps'].quantile(0.95)
            },
            "gpu_metrics": {
                "max_gpu_temp": df['gpu_temp'].max(),
                "average_gpu_temp": df['gpu_temp'].mean(),
                "max_power_draw": df['power_draw'].max(),
                "average_power_draw": df['power_draw'].mean(),
                "max_gpu_util": df['gpu_util'].max(),
                "average_gpu_util": df['gpu_util'].mean(),
                "max_memory_util": df['memory_util'].max(),
                "average_memory_util": df['memory_util'].mean()
            },
            "system_metrics": {
                "max_cpu_util": df['cpu_percent'].max(),
                "average_cpu_util": df['cpu_percent'].mean(),
                "max_memory_util": df['memory_percent'].max(),
                "average_memory_util": df['memory_percent'].mean()
            },
            "hourly_performance": self.hourly_performance
        }
        
        # æ€§èƒ½ç¨³å®šæ€§åˆ†æ
        stable_period = df[df['elapsed_hours'] >= 0.5]  # æ’é™¤å‰30åˆ†é’Ÿçš„çƒ­èº«æœŸ
        if len(stable_period) > 0:
            report["stability_analysis"] = {
                "stable_avg_rps": stable_period['batch_rps'].mean(),
                "stable_avg_tps": stable_period['batch_tps'].mean(),
                "stable_std_rps": stable_period['batch_rps'].std(),
                "stable_std_tps": stable_period['batch_tps'].std(),
                "cv_rps": stable_period['batch_rps'].std() / stable_period['batch_rps'].mean() if stable_period['batch_rps'].mean() > 0 else 0,
                "cv_tps": stable_period['batch_tps'].std() / stable_period['batch_tps'].mean() if stable_period['batch_tps'].mean() > 0 else 0
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_filename = f"comprehensive_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        self.print_report_summary(report)
        
        return report_filename
    
    def print_report_summary(self, report):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "="*80)
        print("é•¿æ—¶é—´å‹åŠ›æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print("="*80)
        
        print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   æŒç»­æ—¶é—´: {report['test_configuration']['duration_hours']} å°æ—¶")
        print(f"   å¹¶å‘å·¥ä½œçº¿ç¨‹: {report['test_configuration']['concurrent_workers']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {report['test_configuration']['batch_size']} è¯·æ±‚/æ‰¹æ¬¡")
        print(f"   å®é™…è¿è¡Œ: {report['test_timing']['actual_duration_hours']:.2f} å°æ—¶")
        
        print(f"\nğŸ“Š è¯·æ±‚ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {report['request_statistics']['total_requests']:,}")
        print(f"   æˆåŠŸç‡: {report['request_statistics']['success_rate']:.2%}")
        print(f"   æ€»ç”ŸæˆToken: {report['request_statistics']['total_tokens']:,}")
        
        print(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡RPS: {report['performance_metrics']['average_batch_rps']:.2f} (Â±{report['performance_metrics']['std_batch_rps']:.2f})")
        print(f"   æœ€é«˜RPS: {report['performance_metrics']['max_batch_rps']:.2f}")
        print(f"   å¹³å‡TPS: {report['performance_metrics']['average_batch_tps']:.2f} (Â±{report['performance_metrics']['std_batch_tps']:.2f})")
        print(f"   æœ€é«˜TPS: {report['performance_metrics']['max_batch_tps']:.2f}")
        print(f"   P95 RPS: {report['performance_metrics']['p95_batch_rps']:.2f}")
        print(f"   P95 TPS: {report['performance_metrics']['p95_batch_tps']:.2f}")
        
        if 'stability_analysis' in report:
            stable = report['stability_analysis']
            print(f"   ç¨³å®šæœŸRPS: {stable['stable_avg_rps']:.2f} (Â±{stable['stable_std_rps']:.2f}, CV: {stable['cv_rps']:.3f})")
            print(f"   ç¨³å®šæœŸTPS: {stable['stable_avg_tps']:.2f} (Â±{stable['stable_std_tps']:.2f}, CV: {stable['cv_tps']:.3f})")
        
        print(f"\nğŸ”¥ GPUæŒ‡æ ‡:")
        print(f"   æ¸©åº¦ - å¹³å‡: {report['gpu_metrics']['average_gpu_temp']:.1f}Â°C, æœ€é«˜: {report['gpu_metrics']['max_gpu_temp']:.1f}Â°C")
        print(f"   åŠŸè€— - å¹³å‡: {report['gpu_metrics']['average_power_draw']:.1f}W, æœ€é«˜: {report['gpu_metrics']['max_power_draw']:.1f}W")
        print(f"   ä½¿ç”¨ç‡ - å¹³å‡: {report['gpu_metrics']['average_gpu_util']:.1f}%, æœ€é«˜: {report['gpu_metrics']['max_gpu_util']:.1f}%")
        
        print(f"\nğŸ’» ç³»ç»ŸæŒ‡æ ‡:")
        print(f"   CPUä½¿ç”¨ç‡ - å¹³å‡: {report['system_metrics']['average_cpu_util']:.1f}%, æœ€é«˜: {report['system_metrics']['max_cpu_util']:.1f}%")
        print(f"   å†…å­˜ä½¿ç”¨ç‡ - å¹³å‡: {report['system_metrics']['average_memory_util']:.1f}%, æœ€é«˜: {report['system_metrics']['max_memory_util']:.1f}%")
        
        # æ¯å°æ—¶æ€§èƒ½è¶‹åŠ¿
        if self.hourly_performance:
            print(f"\nğŸ“ˆ æ¯å°æ—¶æ€§èƒ½è¶‹åŠ¿:")
            for hour_data in self.hourly_performance:
                print(f"   ç¬¬{hour_data['hour']}å°æ—¶: RPS={hour_data['avg_rps']:.2f}, TPS={hour_data['avg_tps']:.2f}, "
                      f"GPU={hour_data['avg_gpu_temp']:.1f}Â°C/{hour_data['avg_gpu_util']:.1f}%")
    
    async def run(self):
        """è¿è¡Œé•¿æ—¶é—´å‹åŠ›æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹é•¿æ—¶é—´å‹åŠ›æµ‹è¯•")
        print(f"â±ï¸  æŒç»­æ—¶é—´: {self.args.duration_hours} å°æ—¶")
        print(f"ğŸ‘¥ å¹¶å‘å·¥ä½œçº¿ç¨‹: {self.args.concurrent_workers}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size} è¯·æ±‚/æ‰¹æ¬¡")
        print(f"ğŸŒ ç›®æ ‡URL: {self.args.url}")
        print(f"ğŸ“ æœ€å¤§ç”Ÿæˆé•¿åº¦: {self.args.max_tokens} tokens")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {self.args.temperature}")
        print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("-" * 80)
        print("æ‰¹æ¬¡ | ç¬æ—¶RPS | ç¬æ—¶TPS | å¹³å‡RPS | å¹³å‡TPS | GPUçŠ¶æ€ | åŠŸè€— | æˆåŠŸç‡")
        print("-" * 80)
        
        self.start_time = time.time()
        self.current_batch_start_time = self.start_time
        self.last_batch_time = self.start_time
        
        # ç”Ÿæˆæµ‹è¯•æç¤º
        test_prompt = self.args.prompt
        if self.args.prompt_length > 0:
            test_prompt += " " + "æµ‹è¯•æ–‡æœ¬" * (self.args.prompt_length // 4)
        
        stop_event = asyncio.Event()
        
        try:
            async with aiohttp.ClientSession() as session:
                # å¯åŠ¨å¤‡ç”¨æŒ‡æ ‡æ”¶é›†å™¨
                backup_task = asyncio.create_task(self.backup_metrics_collector(stop_event))
                
                # å¯åŠ¨æ¯å°æ—¶æ±‡æ€»æ”¶é›†å™¨
                hourly_task = asyncio.create_task(self.hourly_summary_collector(stop_event))
                
                # å¯åŠ¨å·¥ä½œçº¿ç¨‹
                worker_tasks = []
                for i in range(self.args.concurrent_workers):
                    task = asyncio.create_task(self.request_worker(session, test_prompt, i, stop_event))
                    worker_tasks.append(task)
                
                # è¿è¡ŒæŒ‡å®šæ—¶é•¿
                await asyncio.sleep(self.args.duration_hours * 3600)
                
                # åœæ­¢æµ‹è¯•å‰è®°å½•æœ€åä¸€ä¸ªæ‰¹æ¬¡
                if self.current_batch_requests > 0:
                    await self.record_batch_performance()
                
                print("\n" + "="*80)
                print("â¹ï¸  æµ‹è¯•æ—¶é—´åˆ°ï¼Œæ­£åœ¨åœæ­¢æµ‹è¯•...")
                stop_event.set()
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡ç»“æŸ
                await asyncio.gather(*worker_tasks, return_exceptions=True)
                await backup_task
                await hourly_task
            
            self.end_time = time.time()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report_filename = self.generate_comprehensive_report()
            
            # ä¿å­˜æ€§èƒ½æ•°æ®æ–‡ä»¶è·¯å¾„
            csv_filename = self.save_batch_metrics_to_csv()
            print(f"\nğŸ’¾ æ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ°: {csv_filename}")
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•ï¼Œæ­£åœ¨åœæ­¢...")
            if self.current_batch_requests > 0:
                await self.record_batch_performance()
            stop_event.set()
            self.end_time = time.time()
            report_filename = self.generate_comprehensive_report()
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")

def main():
    parser = argparse.ArgumentParser(description='vLLM é•¿æ—¶é—´å‹åŠ›æµ‹è¯•å·¥å…· - æœ€ç»ˆç‰ˆ')
    parser.add_argument('--duration-hours', type=float, default=3.0, help='æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰')
    parser.add_argument('--concurrent-workers', type=int, default=20, help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--batch-size', type=int, default=10, help='æ¯æ‰¹è¯·æ±‚æ•°é‡')
    parser.add_argument('--prompt-length', type=int, default=256, help='æç¤ºè¯é•¿åº¦')
    parser.add_argument('--max-tokens', type=int, default=500, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--temperature', type=float, default=0.7, help='æ¸©åº¦å‚æ•°')
    parser.add_argument('--url', type=str, default='http://localhost:8000/generate', help='vLLMæœåŠ¡å™¨URL')
    parser.add_argument('--prompt', type=str, default='è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬åŸç†å’Œåº”ç”¨é¢†åŸŸã€‚', help='æµ‹è¯•æç¤ºè¯')
    
    args = parser.parse_args()
    
    benchmark = FinalLongRunningBenchmark(args)
    asyncio.run(benchmark.run())

if __name__ == "__main__":
    main()
